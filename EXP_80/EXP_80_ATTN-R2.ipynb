{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os \n",
    "os.chdir('..')\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.callbacks import TrackerCallback\n",
    "from fastai.callbacks import CSVLogger\n",
    "from fastai.data_block import _maybe_squeeze\n",
    "from fastai.callbacks import *\n",
    "from utils.mxresnet import *\n",
    "from utils.ranger import *\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from joblib import load, dump\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "NAME = 'EXP_80'\n",
    "SUFFIX =f'SERESNEXT80_1CH_MISH_R2{FOLD}'\n",
    "PATH = Path('../../../bengaliai')\n",
    "SZ = 224\n",
    "BS = 128\n",
    "NFOLDS = 5 #keep the same split as the initial dataset\n",
    "SEED = 2019\n",
    "TRAIN_IMG = PATH/'img_trn_224'\n",
    "LABELS = PATH/'train.csv'\n",
    "\n",
    "os.chdir(NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strt_split(x, y, n_folds=5, random_seed = 42, path=Path('')):  \n",
    "    try: \n",
    "        val_name = load('val_idx.joblib')\n",
    "    except:\n",
    "        skf = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        val_name = [(val_idx, trn_idx) for trn_idx, val_idx in skf.split(x, y)]\n",
    "        dump(val_name,'val_idx.joblib')\n",
    "    return val_name\n",
    "\n",
    "def multi_strt_split(x, y, n_folds=5, random_seed = 42, path=Path('')):  \n",
    "    try: \n",
    "        val_name = load('val_idx.joblib')\n",
    "    except:\n",
    "        skf = MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        val_name = [(val_idx, trn_idx) for trn_idx, val_idx in skf.split(x, y)]\n",
    "        dump(val_name,'val_idx.joblib')\n",
    "    return val_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(str(LABELS))\n",
    "nunique = list(df.nunique())[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bukharih/.conda/envs/hb/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.values' or 'np.asarray(..)' instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "val_idx = multi_strt_split(df['image_id'], df[df.columns[1:-1]].get_values())[FOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet_stats\n",
    "data = (ImageList.from_df(df, path='', folder=TRAIN_IMG, suffix='.png', \n",
    "        cols='image_id', convert_mode='L')\n",
    "        .split_by_idxs(valid_idx=val_idx[0], train_idx=val_idx[1])\n",
    "        .label_from_df(cols=['grapheme_root','vowel_diacritic','consonant_diacritic'])\n",
    "        .transform(get_transforms(do_flip=False,max_warp=0.2, max_zoom=0.95, max_rotate=20, xtra_tfms=[cutout(n_holes=(5, 25), length=(10, 30), p=.5), squish(scale=0.66, p=0.3)]), size=SZ, padding_mode='zeros')\n",
    "        .databunch(bs=BS)).normalize(([0.0786], [0.2130]))\n",
    "\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0786], [0.213])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ = data.stats\n",
    "stats_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
    "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias: conv.bias.data.zero_()\n",
    "    return spectral_norm(conv)\n",
    "\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, n_in:int, ks=1, sym=False):#, n_out:int):\n",
    "        super().__init__() \n",
    "        self.conv = conv1d(n_in, n_in, ks, padding=ks//2, bias=False)      \n",
    "        self.gamma = nn.Parameter(tensor([0.]))\n",
    "        self.sym = sym\n",
    "        self.n_in = n_in\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.sym:\n",
    "            # symmetry hack by https://github.com/mgrankin\n",
    "            c = self.conv.weight.view(self.n_in,self.n_in)\n",
    "            c = (c + c.t())/2\n",
    "            self.conv.weight = c.view(self.n_in,self.n_in,1)      \n",
    "        size = x.size()  \n",
    "        x = x.view(*size[:2],-1)   # (C,N)\n",
    "        convx = self.conv(x)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
    "        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())   # (C,N) * (N,C) = (C,C)   => O(NC^2)\n",
    "        o = torch.bmm(xxT, convx)   # (C,C) * (C,N) = (C,N)   => O(NC^2)\n",
    "        o = self.gamma * o + x\n",
    "        return o.view(*size).contiguous()   \n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, nc, n, ps=0.2):\n",
    "        super().__init__()\n",
    "        layers =  [SimpleSelfAttention(nc), \n",
    "                  conv_layer(nc, nc//2, ks=1),\n",
    "                  conv_layer(nc//2, nc//2, ks=3), \n",
    "                  conv_layer(nc//2, nc, use_activ=False, ks=1)]+ [AdaptiveConcatPool2d(), Flatten(), nn.Linear(nc*2, n)]\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self._init_weight()\n",
    "        \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1.0)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, arch, cut, nc, n=nunique, pre=False, ps=0.5, activ_mish=True):\n",
    "        super().__init__()\n",
    "        self.body=nn.Sequential(*list(arch.children())[:cut])\n",
    "        self.head1 = Head(nc,n[0])\n",
    "        self.head2 = Head(nc,n[1])\n",
    "        self.head3 = Head(nc,n[2])\n",
    "        if activ_mish: \n",
    "            to_Mish(self.body) \n",
    "            to_Mish(self.head1), to_Mish(self.head2), to_Mish(self.head3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x1 = self.head1(x)\n",
    "        x2 = self.head2(x)\n",
    "        x3 = self.head3(x)\n",
    "        return x1, x2, x3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = pretrainedmodels.se_resnext50_32x4d(num_classes=1000)\n",
    "arch = list(arch.children())\n",
    "w = arch[0][0].weight\n",
    "arch[0][0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=2, bias=False)\n",
    "arch[0][0].weight = nn.Parameter(torch.mean(w, dim=1, keepdim=True))\n",
    "arch = nn.Sequential(*arch)\n",
    "model = CustomHead(arch=arch, cut=-2, nc=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomHead(\n",
       "  (body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Mish()\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (3): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (3): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (4): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (5): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): Mish()\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): Mish()\n",
       "          (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head1): Head(\n",
       "    (fc): Sequential(\n",
       "      (0): SimpleSelfAttention(\n",
       "        (conv): Conv1d(2048, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): AdaptiveConcatPool2d(\n",
       "        (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "        (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "      )\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=4096, out_features=168, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head2): Head(\n",
       "    (fc): Sequential(\n",
       "      (0): SimpleSelfAttention(\n",
       "        (conv): Conv1d(2048, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): AdaptiveConcatPool2d(\n",
       "        (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "        (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "      )\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=4096, out_features=11, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head3): Head(\n",
       "    (fc): Sequential(\n",
       "      (0): SimpleSelfAttention(\n",
       "        (conv): Conv1d(2048, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): Mish()\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): AdaptiveConcatPool2d(\n",
       "        (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "        (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "      )\n",
       "      (5): Flatten()\n",
       "      (6): Linear(in_features=4096, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_combine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target,reduction='mean'):\n",
    "        x1,x2,x3 = input\n",
    "        x1,x2,x3 = x1.float(),x2.float(),x3.float()\n",
    "        y = target.long()\n",
    "        return 0.7*F.cross_entropy(x1,y[:,0],reduction=reduction) + 0.1*F.cross_entropy(x2,y[:,1],reduction=reduction) + \\\n",
    "          0.2*F.cross_entropy(x3,y[:,2],reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpLoss(Module):\n",
    "    \"Adapt the loss function `crit` to go with mixup.\"\n",
    "    \n",
    "    def __init__(self, crit, reduction='mean'):\n",
    "        super().__init__()\n",
    "        if hasattr(crit, 'reduction'): \n",
    "            self.crit = crit\n",
    "            self.old_red = crit.reduction\n",
    "            setattr(self.crit, 'reduction', 'none')\n",
    "        else: \n",
    "            self.crit = partial(crit, reduction='none')\n",
    "            self.old_crit = crit\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        if len(target.shape) == 2 and target.shape[1] == 7:\n",
    "            loss1, loss2 = self.crit(output,target[:,0:3].long()), self.crit(output,target[:,3:6].long())\n",
    "            d = loss1 * target[:,-1] + loss2 * (1-target[:,-1])\n",
    "        else:  d = self.crit(output, target)\n",
    "        if self.reduction == 'mean':    return d.mean()\n",
    "        elif self.reduction == 'sum':   return d.sum()\n",
    "        return d\n",
    "    \n",
    "    def get_old(self):\n",
    "        if hasattr(self, 'old_crit'):  return self.old_crit\n",
    "        elif hasattr(self, 'old_red'): \n",
    "            setattr(self.crit, 'reduction', self.old_red)\n",
    "            return self.crit\n",
    "\n",
    "class MixUpCallback(LearnerCallback):\n",
    "    \"Callback that creates the mixed-up input and target.\"\n",
    "    def __init__(self, learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True):\n",
    "        super().__init__(learn)\n",
    "        self.alpha,self.stack_x,self.stack_y = alpha,stack_x,stack_y\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        \"Applies mixup to `last_input` and `last_target` if `train`.\"\n",
    "        if not train: return\n",
    "        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))\n",
    "        lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n",
    "        lambd = last_input.new(lambd)\n",
    "        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)\n",
    "        x1, y1 = last_input[shuffle], last_target[shuffle]\n",
    "        if self.stack_x:\n",
    "            new_input = [last_input, last_input[shuffle], lambd]\n",
    "        else: \n",
    "            out_shape = [lambd.size(0)] + [1 for _ in range(len(x1.shape) - 1)]\n",
    "            new_input = (last_input * lambd.view(out_shape) + x1 * (1-lambd).view(out_shape))\n",
    "        if self.stack_y:\n",
    "            new_target = torch.cat([last_target.float(), y1.float(), lambd[:,None].float()], 1)\n",
    "        else:\n",
    "            if len(last_target.shape) == 2:\n",
    "                lambd = lambd.unsqueeze(1).float()\n",
    "            new_target = last_target.float() * lambd + y1.float() * (1-lambd)\n",
    "        return {'last_input': new_input, 'last_target': new_target}  \n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric_idx(Callback):\n",
    "    def __init__(self, idx, average='macro'):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "        self.n_classes = 0\n",
    "        self.average = average\n",
    "        self.cm = None\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.cm = None\n",
    "    \n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        last_output = last_output[self.idx]\n",
    "        last_target = last_target[:,self.idx]\n",
    "        preds = last_output.argmax(-1).view(-1).cpu()\n",
    "        targs = last_target.long().cpu()\n",
    "        \n",
    "        if self.n_classes == 0:\n",
    "            self.n_classes = last_output.shape[-1]\n",
    "            self.x = torch.arange(0, self.n_classes)\n",
    "        cm = ((preds==self.x[:, None]) & (targs==self.x[:, None, None])) \\\n",
    "          .sum(dim=2, dtype=torch.float32)\n",
    "        if self.cm is None: self.cm =  cm\n",
    "        else:               self.cm += cm\n",
    "\n",
    "    def _weights(self, avg:str):\n",
    "        if self.n_classes != 2 and avg == \"binary\":\n",
    "            avg = self.average = \"macro\"\n",
    "            warn(\"average=`binary` was selected for a non binary case. \\\n",
    "                 Value for average has now been set to `macro` instead.\")\n",
    "        if avg == \"binary\":\n",
    "            if self.pos_label not in (0, 1):\n",
    "                self.pos_label = 1\n",
    "                warn(\"Invalid value for pos_label. It has now been set to 1.\")\n",
    "            if self.pos_label == 1: return Tensor([0,1])\n",
    "            else: return Tensor([1,0])\n",
    "        elif avg == \"micro\": return self.cm.sum(dim=0) / self.cm.sum()\n",
    "        elif avg == \"macro\": return torch.ones((self.n_classes,)) / self.n_classes\n",
    "        elif avg == \"weighted\": return self.cm.sum(dim=1) / self.cm.sum()\n",
    "        \n",
    "    def _recall(self):\n",
    "        rec = torch.diag(self.cm) / (self.cm.sum(dim=1) + self.eps)\n",
    "        if self.average is None: return rec\n",
    "        else:\n",
    "            if self.average == \"micro\": weights = self._weights(avg=\"weighted\")\n",
    "            else: weights = self._weights(avg=self.average)\n",
    "            return (rec * weights).sum()\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, self._recall())\n",
    "    \n",
    "Metric_grapheme = partial(Metric_idx,0)\n",
    "Metric_vowel = partial(Metric_idx,1)\n",
    "Metric_consonant = partial(Metric_idx,2)\n",
    "\n",
    "class Metric_tot(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.grapheme = Metric_idx(0)\n",
    "        self.vowel = Metric_idx(1)\n",
    "        self.consonant = Metric_idx(2)\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.grapheme.on_epoch_begin(**kwargs)\n",
    "        self.vowel.on_epoch_begin(**kwargs)\n",
    "        self.consonant.on_epoch_begin(**kwargs)\n",
    "    \n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        self.grapheme.on_batch_end(last_output, last_target, **kwargs)\n",
    "        self.vowel.on_batch_end(last_output, last_target, **kwargs)\n",
    "        self.consonant.on_batch_end(last_output, last_target, **kwargs)\n",
    "        \n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, 0.5*self.grapheme._recall() +\n",
    "                0.25*self.vowel._recall() + 0.25*self.consonant._recall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn = Learner(data, model, loss_func=Loss_combine(),opt_func = Over9000,\n",
    "        metrics=[Metric_grapheme(),Metric_vowel(),Metric_consonant(),Metric_tot()])\n",
    "learn.model = nn.DataParallel(learn.model)\n",
    "learn.load('EXP_80_SERESNEXT80_1CH_MISH_0_0')\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>metric_idx</th>\n",
       "      <th>metric_idx</th>\n",
       "      <th>metric_idx</th>\n",
       "      <th>metric_tot</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.015473</td>\n",
       "      <td>0.115648</td>\n",
       "      <td>0.952307</td>\n",
       "      <td>0.986243</td>\n",
       "      <td>0.983744</td>\n",
       "      <td>0.968650</td>\n",
       "      <td>21:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.913570</td>\n",
       "      <td>0.121669</td>\n",
       "      <td>0.951967</td>\n",
       "      <td>0.981795</td>\n",
       "      <td>0.978876</td>\n",
       "      <td>0.966151</td>\n",
       "      <td>21:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.840744</td>\n",
       "      <td>0.120441</td>\n",
       "      <td>0.952895</td>\n",
       "      <td>0.976399</td>\n",
       "      <td>0.984729</td>\n",
       "      <td>0.966730</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.775184</td>\n",
       "      <td>0.118991</td>\n",
       "      <td>0.956474</td>\n",
       "      <td>0.975708</td>\n",
       "      <td>0.984238</td>\n",
       "      <td>0.968224</td>\n",
       "      <td>21:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.714497</td>\n",
       "      <td>0.108181</td>\n",
       "      <td>0.955772</td>\n",
       "      <td>0.984850</td>\n",
       "      <td>0.981267</td>\n",
       "      <td>0.969415</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.659640</td>\n",
       "      <td>0.151115</td>\n",
       "      <td>0.953275</td>\n",
       "      <td>0.957612</td>\n",
       "      <td>0.983113</td>\n",
       "      <td>0.961818</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.648525</td>\n",
       "      <td>0.104040</td>\n",
       "      <td>0.956599</td>\n",
       "      <td>0.988469</td>\n",
       "      <td>0.982715</td>\n",
       "      <td>0.971095</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.614528</td>\n",
       "      <td>0.102514</td>\n",
       "      <td>0.955748</td>\n",
       "      <td>0.985803</td>\n",
       "      <td>0.978952</td>\n",
       "      <td>0.969063</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.590008</td>\n",
       "      <td>0.103837</td>\n",
       "      <td>0.959467</td>\n",
       "      <td>0.981402</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.970875</td>\n",
       "      <td>21:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.568960</td>\n",
       "      <td>0.259433</td>\n",
       "      <td>0.958796</td>\n",
       "      <td>0.922301</td>\n",
       "      <td>0.984370</td>\n",
       "      <td>0.956066</td>\n",
       "      <td>21:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.552294</td>\n",
       "      <td>0.099017</td>\n",
       "      <td>0.958072</td>\n",
       "      <td>0.987831</td>\n",
       "      <td>0.980670</td>\n",
       "      <td>0.971161</td>\n",
       "      <td>21:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.534383</td>\n",
       "      <td>0.096759</td>\n",
       "      <td>0.959745</td>\n",
       "      <td>0.988017</td>\n",
       "      <td>0.984766</td>\n",
       "      <td>0.973068</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.527981</td>\n",
       "      <td>0.094782</td>\n",
       "      <td>0.961091</td>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.985147</td>\n",
       "      <td>0.973845</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.518063</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>0.960447</td>\n",
       "      <td>0.976352</td>\n",
       "      <td>0.984285</td>\n",
       "      <td>0.970383</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.506221</td>\n",
       "      <td>0.092471</td>\n",
       "      <td>0.961041</td>\n",
       "      <td>0.986124</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>0.973301</td>\n",
       "      <td>21:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.502006</td>\n",
       "      <td>0.091096</td>\n",
       "      <td>0.960816</td>\n",
       "      <td>0.988635</td>\n",
       "      <td>0.985243</td>\n",
       "      <td>0.973877</td>\n",
       "      <td>21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.501990</td>\n",
       "      <td>0.090916</td>\n",
       "      <td>0.961274</td>\n",
       "      <td>0.988860</td>\n",
       "      <td>0.985207</td>\n",
       "      <td>0.974154</td>\n",
       "      <td>21:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>0.125163</td>\n",
       "      <td>0.961717</td>\n",
       "      <td>0.957992</td>\n",
       "      <td>0.984547</td>\n",
       "      <td>0.966493</td>\n",
       "      <td>21:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.493086</td>\n",
       "      <td>0.092294</td>\n",
       "      <td>0.962063</td>\n",
       "      <td>0.986023</td>\n",
       "      <td>0.985057</td>\n",
       "      <td>0.973802</td>\n",
       "      <td>21:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.496531</td>\n",
       "      <td>0.105524</td>\n",
       "      <td>0.961389</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.984920</td>\n",
       "      <td>0.969716</td>\n",
       "      <td>21:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bukharih/.conda/envs/hb/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with metric_tot value: 0.9686499834060669.\n",
      "Better model found at epoch 4 with metric_tot value: 0.9694154262542725.\n",
      "Better model found at epoch 6 with metric_tot value: 0.9710954427719116.\n",
      "Better model found at epoch 10 with metric_tot value: 0.9711611270904541.\n",
      "Better model found at epoch 11 with metric_tot value: 0.9730681777000427.\n",
      "Better model found at epoch 12 with metric_tot value: 0.9738447666168213.\n",
      "Better model found at epoch 15 with metric_tot value: 0.9738773703575134.\n",
      "Better model found at epoch 16 with metric_tot value: 0.9741537570953369.\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "lr = 1e-3\n",
    "learn.fit_one_cycle(20, lr, wd=1e-2,  pct_start=0.0,  div_factor=100,\n",
    "                    callbacks = [SaveModelCallback(learn, every='improvement', monitor='metric_tot', name = f'{NAME}_{SUFFIX}_{FOLD}'), MixUpCallback(learn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
